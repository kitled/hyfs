{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d6dfc4",
   "metadata": {
    "hide_input": true,
    "time_run": "5:15:05p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created dummy filesystem at /app/data/dev/hyfs/test/fs\n",
      "âœ“ Total files created: 60\n",
      "âœ“ Includes edge cases: spaces, special chars, hidden files, deep nesting, various extensions\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Root directory\n",
    "root = Path(\"/app/data/dev/hyfs/test/fs\")\n",
    "\n",
    "# Create the root directory\n",
    "root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the filesystem structure with lots of variety and edge cases\n",
    "filesystem = {\n",
    "    # Regular files at root\n",
    "    \"README.md\": \"# Test Project\\n\",\n",
    "    \"config.json\": '{\"version\": \"1.0\"}\\n',\n",
    "    \".gitignore\": \"*.pyc\\n__pycache__/\\n\",\n",
    "    \".env\": \"SECRET_KEY=test123\\n\",\n",
    "    \"requirements.txt\": \"numpy==1.24.0\\npandas>=2.0.0\\n\",\n",
    "    \n",
    "    # Source code directory\n",
    "    \"src/main.py\": \"def main():\\n    pass\\n\",\n",
    "    \"src/utils.py\": \"# Utilities\\n\",\n",
    "    \"src/__init__.py\": \"\",\n",
    "    \"src/models/user.py\": \"class User:\\n    pass\\n\",\n",
    "    \"src/models/__init__.py\": \"\",\n",
    "    \"src/models/product.py\": \"class Product:\\n    pass\\n\",\n",
    "    \n",
    "    # Tests directory\n",
    "    \"tests/test_main.py\": \"def test_main():\\n    assert True\\n\",\n",
    "    \"tests/__init__.py\": \"\",\n",
    "    \"tests/fixtures/data.json\": '{\"test\": \"data\"}\\n',\n",
    "    \"tests/fixtures/sample.csv\": \"id,name,value\\n1,test,100\\n\",\n",
    "    \n",
    "    # Data directory with various file types\n",
    "    \"data/raw/dataset_2023.csv\": \"col1,col2,col3\\n1,2,3\\n\",\n",
    "    \"data/raw/dataset_2024.csv\": \"col1,col2,col3\\n4,5,6\\n\",\n",
    "    \"data/processed/cleaned_data.parquet\": b\"fake parquet data\",\n",
    "    \"data/processed/features.pkl\": b\"fake pickle data\",\n",
    "    \"data/images/photo1.jpg\": b\"fake jpg data\",\n",
    "    \"data/images/photo2.png\": b\"fake png data\",\n",
    "    \"data/images/thumbnails/thumb1.jpg\": b\"fake thumbnail\",\n",
    "    \n",
    "    # Documentation\n",
    "    \"docs/index.html\": \"<html><body>Docs</body></html>\\n\",\n",
    "    \"docs/api/endpoints.md\": \"# API Endpoints\\n\",\n",
    "    \"docs/api/authentication.md\": \"# Auth\\n\",\n",
    "    \"docs/guides/getting-started.pdf\": b\"fake pdf data\",\n",
    "    \n",
    "    # Configuration files\n",
    "    \"config/development.yaml\": \"debug: true\\n\",\n",
    "    \"config/production.yaml\": \"debug: false\\n\",\n",
    "    \"config/database.ini\": \"[database]\\nhost=localhost\\n\",\n",
    "    \n",
    "    # Build artifacts\n",
    "    \"build/output.js\": \"console.log('built');\\n\",\n",
    "    \"build/styles.css\": \"body { margin: 0; }\\n\",\n",
    "    \"dist/bundle.min.js\": \"!function(){console.log('minified')}();\\n\",\n",
    "    \n",
    "    # Edge cases\n",
    "    \"files with spaces/document 1.txt\": \"Content with spaces\\n\",\n",
    "    \"files with spaces/my file (copy).docx\": b\"fake docx\",\n",
    "    \"special-chars/file@2024.txt\": \"File with @ symbol\\n\",\n",
    "    \"special-chars/data#1.csv\": \"test,data\\n\",\n",
    "    \"special-chars/report_v2.1.pdf\": b\"fake pdf\",\n",
    "    \"multiple.dots.in.name.txt\": \"Multiple dots\\n\",\n",
    "    \"UPPERCASE.TXT\": \"UPPERCASE FILE\\n\",\n",
    "    \"MixedCase.TxT\": \"Mixed case extension\\n\",\n",
    "    \n",
    "    # Hidden files and directories\n",
    "    \".hidden/secret.txt\": \"Hidden content\\n\",\n",
    "    \".hidden/.config\": \"hidden config\\n\",\n",
    "    \".cache/temp1.tmp\": \"cache data\\n\",\n",
    "    \n",
    "    # Empty directory (will create separately)\n",
    "    \"empty_dir/.keep\": \"\",\n",
    "    \n",
    "    # Deep nesting\n",
    "    \"a/b/c/d/e/deep_file.txt\": \"Very nested\\n\",\n",
    "    \n",
    "    # Various extensions\n",
    "    \"scripts/deploy.sh\": \"#!/bin/bash\\necho 'deploying'\\n\",\n",
    "    \"scripts/backup.bat\": \"@echo off\\necho backing up\\n\",\n",
    "    \"notebooks/analysis.ipynb\": '{\"cells\": []}\\n',\n",
    "    \"media/video.mp4\": b\"fake video data\",\n",
    "    \"media/audio.mp3\": b\"fake audio data\",\n",
    "    \"archives/backup.zip\": b\"fake zip data\",\n",
    "    \"archives/old_data.tar.gz\": b\"fake tar.gz data\",\n",
    "    \n",
    "    # Files with no extension\n",
    "    \"LICENSE\": \"MIT License\\n\",\n",
    "    \"Makefile\": \"all:\\n\\techo 'building'\\n\",\n",
    "    \"Dockerfile\": \"FROM python:3.11\\n\",\n",
    "    \n",
    "    # Very long filename\n",
    "    \"long_filename_that_goes_on_and_on_and_on_to_test_length_limits.txt\": \"Long name\\n\",\n",
    "    \n",
    "    # Numeric filenames\n",
    "    \"logs/2024-01-01.log\": \"[INFO] Log entry\\n\",\n",
    "    \"logs/2024-01-02.log\": \"[ERROR] Error entry\\n\",\n",
    "    \"reports/001_report.txt\": \"Report 1\\n\",\n",
    "    \"reports/002_report.txt\": \"Report 2\\n\",\n",
    "}\n",
    "\n",
    "# Create all files and directories\n",
    "for filepath, content in filesystem.items():\n",
    "    full_path = root / filepath\n",
    "    full_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if isinstance(content, bytes):\n",
    "        full_path.write_bytes(content)\n",
    "    else:\n",
    "        full_path.write_text(content)\n",
    "\n",
    "# Create a truly empty directory\n",
    "(root / \"truly_empty\").mkdir(exist_ok=True)\n",
    "\n",
    "# Create another empty nested directory\n",
    "(root / \"temp/cache/empty\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"âœ“ Created dummy filesystem at {root}\")\n",
    "print(f\"âœ“ Total files created: {len(filesystem)}\")\n",
    "print(f\"âœ“ Includes edge cases: spaces, special chars, hidden files, deep nesting, various extensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe030eb0",
   "metadata": {
    "collapsed": true,
    "time_run": "5:15:05p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "â”œâ”€ fs\r\n",
      "|  â”œâ”€ config.json\r\n",
      "|  â”œâ”€ .hidden\r\n",
      "|  |  â”œâ”€ secret.txt\r\n",
      "|  |  â”œâ”€ .config\r\n",
      "|  â”œâ”€ src\r\n",
      "|  |  â”œâ”€ models\r\n",
      "|  |  |  â”œâ”€ product.py\r\n",
      "|  |  |  â”œâ”€ __init__.py\r\n",
      "|  |  |  â”œâ”€ user.py\r\n",
      "|  |  â”œâ”€ utils.py\r\n",
      "|  |  â”œâ”€ __init__.py\r\n",
      "|  |  â”œâ”€ main.py\r\n",
      "|  â”œâ”€ dist\r\n",
      "|  |  â”œâ”€ bundle.min.js\r\n",
      "|  â”œâ”€ tests\r\n",
      "|  |  â”œâ”€ test_main.py\r\n",
      "|  |  â”œâ”€ __init__.py\r\n",
      "|  |  â”œâ”€ fixtures\r\n",
      "|  |  |  â”œâ”€ data.json\r\n",
      "|  |  |  â”œâ”€ sample.csv\r\n",
      "|  â”œâ”€ build\r\n",
      "|  |  â”œâ”€ styles.css\r\n",
      "|  |  â”œâ”€ output.js\r\n",
      "|  â”œâ”€ reports\r\n",
      "|  |  â”œâ”€ 002_report.txt\r\n",
      "|  |  â”œâ”€ 001_report.txt\r\n",
      "|  â”œâ”€ config\r\n",
      "|  |  â”œâ”€ development.yaml\r\n",
      "|  |  â”œâ”€ production.yaml\r\n",
      "|  |  â”œâ”€ database.ini\r\n",
      "|  â”œâ”€ .gitignore\r\n",
      "|  â”œâ”€ requirements.txt\r\n",
      "|  â”œâ”€ media\r\n",
      "|  |  â”œâ”€ video.mp4\r\n",
      "|  |  â”œâ”€ audio.mp3\r\n",
      "|  â”œâ”€ docs\r\n",
      "|  |  â”œâ”€ api\r\n",
      "|  |  |  â”œâ”€ endpoints.md\r\n",
      "|  |  |  â”œâ”€ authentication.md\r\n",
      "|  |  â”œâ”€ index.html\r\n",
      "|  |  â”œâ”€ guides\r\n",
      "|  |  |  â”œâ”€ getting-started.pdf\r\n",
      "|  â”œâ”€ files with spaces\r\n",
      "|  |  â”œâ”€ document 1.txt\r\n",
      "|  |  â”œâ”€ my file (copy).docx\r\n",
      "|  â”œâ”€ UPPERCASE.TXT\r\n",
      "|  â”œâ”€ MixedCase.TxT\r\n",
      "|  â”œâ”€ README.md\r\n",
      "|  â”œâ”€ .env\r\n",
      "|  â”œâ”€ multiple.dots.in.name.txt\r\n",
      "|  â”œâ”€ archives\r\n",
      "|  |  â”œâ”€ backup.zip\r\n",
      "|  |  â”œâ”€ old_data.tar.gz\r\n",
      "|  â”œâ”€ truly_empty\r\n",
      "|  â”œâ”€ Makefile\r\n",
      "|  â”œâ”€ Dockerfile\r\n",
      "|  â”œâ”€ a\r\n",
      "|  |  â”œâ”€ b\r\n",
      "|  |  |  â”œâ”€ c\r\n",
      "|  |  |  |  â”œâ”€ d\r\n",
      "|  |  |  |  |  â”œâ”€ e\r\n",
      "|  |  |  |  |  |  â”œâ”€ deep_file.txt\r\n",
      "|  â”œâ”€ notebooks\r\n",
      "|  |  â”œâ”€ analysis.ipynb\r\n",
      "|  â”œâ”€ logs\r\n",
      "|  |  â”œâ”€ 2024-01-01.log\r\n",
      "|  |  â”œâ”€ 2024-01-02.log\r\n",
      "|  â”œâ”€ empty_dir\r\n",
      "|  |  â”œâ”€ .keep\r\n",
      "|  â”œâ”€ long_filename_that_goes_on_and_on_and_on_to_test_length_limits.txt\r\n",
      "|  â”œâ”€ temp\r\n",
      "|  |  â”œâ”€ cache\r\n",
      "|  |  |  â”œâ”€ empty\r\n",
      "|  â”œâ”€ scripts\r\n",
      "|  |  â”œâ”€ backup.bat\r\n",
      "|  |  â”œâ”€ deploy.sh\r\n",
      "|  â”œâ”€ data\r\n",
      "|  |  â”œâ”€ raw\r\n",
      "|  |  |  â”œâ”€ dataset_2023.csv\r\n",
      "|  |  |  â”œâ”€ dataset_2024.csv\r\n",
      "|  |  â”œâ”€ images\r\n",
      "|  |  |  â”œâ”€ thumbnails\r\n",
      "|  |  |  |  â”œâ”€ thumb1.jpg\r\n",
      "|  |  |  â”œâ”€ photo1.jpg\r\n",
      "|  |  |  â”œâ”€ photo2.png\r\n",
      "|  |  â”œâ”€ processed\r\n",
      "|  |  |  â”œâ”€ cleaned_data.parquet\r\n",
      "|  |  |  â”œâ”€ features.pkl\r\n",
      "|  â”œâ”€ special-chars\r\n",
      "|  |  â”œâ”€ data#1.csv\r\n",
      "|  |  â”œâ”€ report_v2.1.pdf\r\n",
      "|  |  â”œâ”€ file@2024.txt\r\n",
      "|  â”œâ”€ LICENSE\r\n",
      "|  â”œâ”€ .cache\r\n",
      "|  |  â”œâ”€ temp1.tmp\r\n",
      "â”œâ”€ tests.py\r\n"
     ]
    }
   ],
   "source": [
    "!find /app/data/dev/hyfs/test -print | sed -e \"s;/app/data/dev/hyfs/test;;\" -e \"s;[^/]*/;|  ;g\" -e \"s;|  \\([^|]\\);â”œâ”€ \\1;\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d85bc8",
   "metadata": {},
   "source": [
    "# HyFS\n",
    "> A Hyper FileSystem\n",
    "\n",
    "Architecture phase: exploration (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0394a",
   "metadata": {
    "collapsed": true,
    "hide_input": true,
    "time_run": "5:15:05p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# HyFS Design Document\r\n",
      "> Hyper FileSystem - A filesystem abstraction with stable identity and semantic relationships\r\n",
      "\r\n",
      "## Vision\r\n",
      "\r\n",
      "HyFS provides stable entity identification and semantic organization for filesystems. Files and directories get persistent identities (eids) that survive renames and moves, enabling tagging, relationship tracking, and multiple views of the same underlying data.\r\n",
      "\r\n",
      "Built for interactive exploration in SolveIT notebooks using fastcore principles.\r\n",
      "\r\n",
      "## Core Philosophy\r\n",
      "\r\n",
      "### Principle of Lean Information Form (LIF)\r\n",
      "\r\n",
      "Information must be expressed in its meaningful form, preserving integrity without requiring decoders. Store semantic structure directly, decide display independently.\r\n",
      "\r\n",
      "**LIF Lemma 1: Separation of Concerns**\r\n",
      "\r\n",
      "Three orthogonal concepts, stored separately:\r\n",
      "1. **Entity storage**: Flat dict `{eid -> node}` (canonical)\r\n",
      "2. **Filesystem hierarchy**: Derived from `path` relationships (view)\r\n",
      "3. **Semantic organization**: Tags and relations (metadata layer)\r\n",
      "\r\n",
      "Don't mix these. Tags are many-to-many mappings. Relations are typed connections. Tree structure is computed on-demand from path relationships.\r\n",
      "\r\n",
      "### The fastcore Way\r\n",
      "\r\n",
      "Methods return transformed data when possible, enabling chaining. `filter()` returns a flat list of nodes, not print output. This separates data transformation from presentation.\r\n",
      "\r\n",
      "**REPL excellence**: Custom `__repr__` methods show just enough information. `HyFS` displays counts with emoji. `FSNode` shows icon, name, and truncated eid. Like `L`, the goal is maximum clarity at a glance.\r\n",
      "\r\n",
      "### Make Side Effects Explicit and Deferrable\r\n",
      "\r\n",
      "Inspired by Git's staging area and ZFS transactions:\r\n",
      "- **Read operations**: Immediate (work directly on flat storage)\r\n",
      "- **Write operations**: Return a Plan/Transaction object that can be inspected, then executed\r\n",
      "- Example: `plan = hyfs.rename(eid, 'newname')` â†’ `plan.preview()` â†’ `plan.execute()`\r\n",
      "\r\n",
      "Provides safety, composability, and clear boundaries between observation and mutation.\r\n",
      "\r\n",
      "## Architecture Decisions\r\n",
      "\r\n",
      "### Flat Storage with Derived Views\r\n",
      "\r\n",
      "**Choice**: Store nodes in flat dict `{eid -> FSNode}`, derive tree structure on-demand.\r\n",
      "\r\n",
      "**Why flat**:\r\n",
      "- O(1) lookup by eid\r\n",
      "- No nested traversal for global operations\r\n",
      "- Tags/relations are just dicts\r\n",
      "- Multiple views from single source\r\n",
      "- Easy serialization\r\n",
      "- Scales better\r\n",
      "\r\n",
      "**Why derived trees**:\r\n",
      "- Tree structure implicit in `path` property\r\n",
      "- Compute hierarchy when needed via `children_index`\r\n",
      "- Display is a view concern, not data concern\r\n",
      "- Can build multiple trees: filesystem, tags, relations\r\n",
      "\r\n",
      "### Multiple Indexes Over Canonical Data\r\n",
      "\r\n",
      "**Pattern**: Maintain several indexes over the flat `nodes` storage:\r\n",
      "- `path_index`: Fast path lookups (O(1))\r\n",
      "- `children_index`: Fast tree construction (O(n) not O(nÂ²))\r\n",
      "- `eid_tags`: Bidirectional tag lookup (O(1) both ways)\r\n",
      "\r\n",
      "All indexes derive from canonical data and can be rebuilt. This is the database approach: primary key + secondary indexes.\r\n",
      "\r\n",
      "### Entity Identification: UUID with xattr\r\n",
      "\r\n",
      "Every node gets a stable `eid` (Entity ID). Try to store UUID in xattr `user.hyfs.uuid`. If xattr unavailable, fall back to deterministic hash of `(st_dev, st_ino, ctime)`.\r\n",
      "\r\n",
      "**Why `eid` not `fid`/`nid`**: Directories are entities tooâ€”structure has semantic meaning. `eid` sits at perfect abstraction level between content (`cid`) and filesystem implementation (`nid`).\r\n",
      "\r\n",
      "**Three-tier approach**:\r\n",
      "1. **Best**: UUID in xattr (true stability)\r\n",
      "2. **Good**: Deterministic hash using `ctime` from xattr (stable if xattr persists)\r\n",
      "3. **Fallback**: Deterministic hash using `st_mtime` (changes on file modification)\r\n",
      "\r\n",
      "### Creation Time: First-Class Metadata\r\n",
      "\r\n",
      "`ctime` is always stored when possible (via `user.hyfs.ctime` xattr), initialized from `st_mtime` on first encounter. This provides:\r\n",
      "- Valuable metadata in its own right (Linux lacks creation time in stat)\r\n",
      "- Stable timestamp for eid computation in fallback scenarios\r\n",
      "- Historical record of when HyFS first saw the entity\r\n",
      "\r\n",
      "**Why `st_mtime` as initial value**: Files are typically created then immediately written. `st_mtime` is the best available approximation of creation time on first encounter.\r\n",
      "\r\n",
      "### Content Addressing: Lazy SHA256 with Two-Tier Cache\r\n",
      "\r\n",
      "`cid` property on `FSNode` uses two-level caching:\r\n",
      "1. Node dict cache (session-only, ~7x speedup on repeated access)\r\n",
      "2. Xattr cache `user.hyfs.cid` (persistent across sessions)\r\n",
      "\r\n",
      "Computed on first access using ZFS-style 64KB streaming chunks. Returns `None` for directories.\r\n",
      "\r\n",
      "**Update mechanism**: `update_cids()` clears both caches and forces recompute from disk. Chainable on `L` for workflows like `hyfs.find('*.py').update_cids()`.\r\n",
      "\r\n",
      "### Path Index: O(1) Lookups\r\n",
      "\r\n",
      "Maintain `path_index = {path: eid}` updated in `add_node()`. Makes `find_by_path()` O(1) instead of O(n) scan.\r\n",
      "\r\n",
      "### Children Index: O(n) Tree Construction\r\n",
      "\r\n",
      "Maintain `children_index = {parent_eid: {child_eids}}` updated in `add_node()`. Tree construction becomes O(n) instead of O(nÂ²) - just walk the index instead of scanning all nodes for each parent.\r\n",
      "\r\n",
      "### Tagging: Singular Operations with Bidirectional Index\r\n",
      "\r\n",
      "Four methods for many-to-many relationships:\r\n",
      "- `tag(eid, tag)` - add one tag to one eid\r\n",
      "- `untag(eid, tag)` - remove one tag from one eid  \r\n",
      "- `tagged(tag)` - get all eids with this tag\r\n",
      "- `tags_of(eid)` - get all tags for this eid\r\n",
      "\r\n",
      "**Bidirectional storage**:\r\n",
      "- `tags[tag] -> {eids}` (forward: tag to entities)\r\n",
      "- `eid_tags[eid] -> {tags}` (reverse: entity to tags)\r\n",
      "\r\n",
      "Makes both directions O(1). Singular operations over variadic (Unix philosophy). Idempotent. Auto-cleanup empty tags. No validation, tags auto-create.\r\n",
      "\r\n",
      "## What HyFS Enables\r\n",
      "\r\n",
      "**Track files across renames**: eid persists through filesystem changes\r\n",
      "\r\n",
      "**Detect duplicates**: Content-based deduplication via `cid`\r\n",
      "\r\n",
      "**Compare trees**: Diff snapshots by eid to find added/removed/moved files\r\n",
      "\r\n",
      "**Semantic relationships**: Tag files, build import graphs, track generation lineage\r\n",
      "\r\n",
      "**Persistent selections**: Tags survive renames, moves, even filesystem boundaries\r\n",
      "\r\n",
      "**Multiple views**: Same data, different perspectivesâ€”tree by path, tree by tags, graph by imports\r\n",
      "\r\n",
      "**Historical metadata**: Creation time tracking even on filesystems that don't support it natively\r\n",
      "\r\n",
      "**Efficient exploration**: REPL-optimized repr methods show exactly what you need at a glance\r\n",
      "\r\n",
      "## Development Principles\r\n",
      "\r\n",
      "- **Vertical space efficiency**: Favor one-liners where clarity isn't sacrificed\r\n",
      "- **Fastcore alignment**: Use `L`, `AttrDict`, `@patch`, `Path`\r\n",
      "- **REPL-driven**: Optimize for tab completion and exploration\r\n",
      "- **Composability over monoliths**: Do one thing well\r\n",
      "- **No premature optimization**: Measure first, but design for scale\r\n",
      "- **No ceremony**: Terse, clear code\r\n",
      "- **Index liberally**: Multiple views over canonical data\r\n",
      "- **Make it chainable**: Return data structures that enable composition\r\n",
      "\r\n",
      "## Current Capabilities\r\n",
      "\r\n",
      "**Scanning**: Walk filesystem, assign stable eids, store in flat structure\r\n",
      "\r\n",
      "**Querying**: Find by eid (O(1)), path (O(1)), pattern (O(n)), predicate (O(n))\r\n",
      "\r\n",
      "**Tree views**: Build hierarchical structure on-demand from flat storage (O(n))\r\n",
      "\r\n",
      "**Tagging**: Many-to-many relationships with O(1) lookups both directions\r\n",
      "\r\n",
      "**Content hashing**: SHA256 with persistent xattr cache and session cache\r\n",
      "\r\n",
      "**Metadata tracking**: Creation time, entity ID, content hash (all via xattr when possible)\r\n",
      "\r\n",
      "**REPL exploration**: Custom repr methods, chainable operations, fastcore idioms\r\n",
      "\r\n",
      "## Future Directions\r\n",
      "\r\n",
      "**Immediate**: \r\n",
      "- Relations API: `hyfs.relate(eid1, 'imports', eid2)`\r\n",
      "- Filtered tree views: Build trees from tagged subsets\r\n",
      "- Write operations as Plans: `plan = hyfs.rename(...); plan.preview(); plan.execute()`\r\n",
      "\r\n",
      "**Medium**: \r\n",
      "- Serialization: Save/load semantic layer (tags, relations)\r\n",
      "- Deduplication: Find and merge identical content\r\n",
      "- Snapshots: Track filesystem state over time\r\n",
      "- Additional metadata properties: `size`, `mtime`, `permissions`\r\n",
      "\r\n",
      "**Long-term**: \r\n",
      "- Multi-host tracking: Follow entities across machines\r\n",
      "- Semantic relationships: Import graphs, generation lineage\r\n",
      "- FastHTML web interface: Visual exploration and manipulation\r\n",
      "- CLI tool: `hyfs tag`, `hyfs find`, `hyfs tree`\r\n",
      "- Smart sync: Content-aware file synchronization\r\n",
      "\r\n",
      "## Design Evolution\r\n",
      "\r\n",
      "HyFS is in exploration phase (iteration 4). Previous attempts tried nested storage, different identity schemes, various tree construction approaches. Current design emerged from use, not upfront planning.\r\n",
      "\r\n",
      "**Key learnings**:\r\n",
      "- Flat storage beats nested for most operations\r\n",
      "- Multiple indexes over canonical data scales better than denormalization\r\n",
      "- Bidirectional indexes eliminate O(n) scans\r\n",
      "- Xattr provides best stability, but graceful fallback essential\r\n",
      "- REPL experience matters as much as API design\r\n",
      "- Metadata like `ctime` valuable independent of implementation needs\r\n",
      "\r\n",
      "**What changed this iteration**:\r\n",
      "- Added `children_index` for O(n) tree construction\r\n",
      "- Added `eid_tags` for O(1) reverse tag lookup\r\n",
      "- Made `ctime` first-class metadata, not just fallback\r\n",
      "- Two-tier caching for `cid` (xattr + node dict)\r\n",
      "- Custom `__repr__` for REPL excellence\r\n",
      "- Centralized xattr helpers for DRY\r\n",
      "\r\n",
      "---\r\n",
      "\r\n",
      "*This is a living document. Update as HyFS evolves.*\r\n",
      "\r\n",
      "Key updates:\r\n",
      "- âœ… Added REPL excellence to \"The fastcore Way\" section\r\n",
      "- âœ… New section on \"Multiple Indexes Over Canonical Data\" pattern\r\n",
      "- âœ… Updated Entity Identification to show three-tier approach\r\n",
      "- âœ… New section on \"Creation Time: First-Class Metadata\"\r\n",
      "- âœ… Updated Content Addressing to show two-tier cache\r\n",
      "- âœ… New section on \"Children Index: O(n) Tree Construction\"\r\n",
      "- âœ… Updated Tagging section to show bidirectional index\r\n",
      "- âœ… Added \"Historical metadata\" and \"Efficient exploration\" to What HyFS Enables\r\n",
      "- âœ… Updated Development Principles with \"Index liberally\" and \"Make it chainable\"\r\n",
      "- âœ… Expanded Current Capabilities to reflect all new features\r\n",
      "- âœ… New \"Design Evolution\" section documenting iteration 4 changes\r\n"
     ]
    }
   ],
   "source": [
    "%cat notes/design.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef7e17e",
   "metadata": {
    "collapsed": true,
    "time_run": "5:15:06p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# HyFS Architecture Document\r\n",
      "> Implementation details, data structures, and algorithms\r\n",
      "\r\n",
      "## Data Structures\r\n",
      "\r\n",
      "### FSNode: AttrDict with Properties\r\n",
      "\r\n",
      "`FSNode` extends `AttrDict` to enable both dict-style (`node['path']`) and attribute-style (`node.path`) access, optimized for REPL exploration.\r\n",
      "\r\n",
      "**Property override challenge**: AttrDict's `__getattr__` intercepts attribute access before class properties. Solution: override `__getattribute__` to check class properties first.\r\n",
      "\r\n",
      "Properties enable lazy computation:\r\n",
      "- `cid`: Computed on first access, cached in node dict\r\n",
      "- Future: `size`, `mtime`, `permissions`\r\n",
      "\r\n",
      "**Custom repr**: Shows file/dir icon (ğŸ“„/ğŸ“), name, and truncated eid (8 chars) for clean REPL display.\r\n",
      "\r\n",
      "### HyFS: Flat Storage Container\r\n",
      "\r\n",
      "```python\r\n",
      "self.nodes = {}              # eid -> FSNode (canonical storage)\r\n",
      "self.path_index = {}         # path -> eid (O(1) lookups)\r\n",
      "self.children_index = {}     # parent_eid -> {child_eids} (O(n) tree construction)\r\n",
      "self.tags = defaultdict(set) # tag_name -> {eid, ...}\r\n",
      "self.eid_tags = defaultdict(set)  # eid -> {tag_name, ...} (bidirectional)\r\n",
      "self.relations = defaultdict(lambda: defaultdict(set))  # eid -> {rel_type -> {eid, ...}}\r\n",
      "```\r\n",
      "\r\n",
      "All lookups are dict-based. Tree structure derived on-demand. Multiple indexes maintained over canonical `nodes` storage.\r\n",
      "\r\n",
      "**Custom repr**: Shows counts with emoji: `HyFS(ğŸ“„ 60 files, ğŸ“ 36 dirs, ğŸ·ï¸ 3 tags)`\r\n",
      "\r\n",
      "## Core Algorithms\r\n",
      "\r\n",
      "### Entity ID Computation\r\n",
      "\r\n",
      "**Three-tier approach** with `ctime` as first-class metadata:\r\n",
      "\r\n",
      "1. **Always** try to store creation time (`user.hyfs.ctime`) using `st_mtime` as initial value\r\n",
      "2. Try read UUID from xattr `user.hyfs.uuid`\r\n",
      "3. If missing, generate UUID v4\r\n",
      "4. Try write to xattr\r\n",
      "5. On xattr failure, fall back to deterministic hash: `SHA256(st_dev:st_ino:ctime)` formatted as UUID\r\n",
      "\r\n",
      "**Key insight**: `ctime` is valuable metadata in its own right, not just for eid fallback. Always stored when possible.\r\n",
      "\r\n",
      "**Stability guarantee**: \r\n",
      "- Best case (xattr): True UUID, survives all filesystem changes\r\n",
      "- Fallback case (no xattr): Deterministic hash stable as long as `ctime` xattr persists\r\n",
      "- Ultimate fallback (no xattr at all): Hash based on `st_mtime`, changes on file modification\r\n",
      "\r\n",
      "### Content ID Computation\r\n",
      "\r\n",
      "**Two-tier caching** for performance:\r\n",
      "\r\n",
      "1. Check xattr `user.hyfs.cid` (persistent across sessions)\r\n",
      "2. If not found, compute SHA256 using 64KB streaming chunks (ZFS-style)\r\n",
      "3. Cache in xattr for next session\r\n",
      "4. Also cache in node dict for current session (~7x speedup on repeated access)\r\n",
      "\r\n",
      "Returns `None` for directories (no standard dir hashing yet).\r\n",
      "\r\n",
      "**Update mechanism**: `update_cids()` method clears both caches (node dict + xattr) to force recompute from disk. Chainable on `L` for workflows like `hyfs.find('*.py').update_cids()`.\r\n",
      "\r\n",
      "### Tree View Construction\r\n",
      "\r\n",
      "**Input**: Root path  \r\n",
      "**Output**: Hierarchical `FSNode` with `children` lists\r\n",
      "\r\n",
      "**Algorithm**:\r\n",
      "1. Look up root node in `path_index` (O(1))\r\n",
      "2. Recursively build tree:\r\n",
      "   - Copy node data into new `FSNode`\r\n",
      "   - If directory: look up children in `children_index` (O(1))\r\n",
      "   - Recursively build each child\r\n",
      "   - Attach children list\r\n",
      "\r\n",
      "**Complexity**: O(n) with children index (previously O(nÂ²)). Index maintained during `add_node()`.\r\n",
      "\r\n",
      "### Filesystem Scanning\r\n",
      "\r\n",
      "**Algorithm**:\r\n",
      "1. Use `Path.rglob('*')` to walk entire tree\r\n",
      "2. For each path:\r\n",
      "   - Compute eid (with xattr/hash fallback, ensures ctime stored)\r\n",
      "   - Determine type (file/dir)\r\n",
      "   - Create `FSNode`\r\n",
      "   - Store in `nodes[eid]` and `path_index[path]`\r\n",
      "   - Update `children_index` if parent exists\r\n",
      "3. Return populated `HyFS` instance\r\n",
      "\r\n",
      "**Performance**: ~1ms per 100 nodes on modern hardware. Metadata (size, mtime) skipped unless requested.\r\n",
      "\r\n",
      "### Tagging: Bidirectional Index\r\n",
      "\r\n",
      "**Data structures**:\r\n",
      "- `tags[tag_name]` â†’ `{eids}` (forward: tag to entities)\r\n",
      "- `eid_tags[eid]` â†’ `{tags}` (reverse: entity to tags)\r\n",
      "\r\n",
      "**Operations**:\r\n",
      "- `tag(eid, tag)`: Update both indexes (O(1))\r\n",
      "- `untag(eid, tag)`: Remove from both, cleanup empty sets (O(1))\r\n",
      "- `tagged(tag)`: Return `tags[tag]` (O(1))\r\n",
      "- `tags_of(eid)`: Return `eid_tags[eid]` (O(1), previously O(n))\r\n",
      "\r\n",
      "**Auto-cleanup**: Empty sets removed to prevent `defaultdict` accumulation.\r\n",
      "\r\n",
      "## Method Patterns\r\n",
      "\r\n",
      "### Querying: Returns vs Side Effects\r\n",
      "\r\n",
      "**Returns data**: `filter()`, `find()`, `get()`, `tagged()`, `tags_of()`, `tree()`\r\n",
      "- Enable chaining: `hyfs.filter(pred1).filter(pred2)`\r\n",
      "- Use `L` for lists (fastcore convention)\r\n",
      "- Return actual references (sets) when mutation might be useful\r\n",
      "\r\n",
      "**Side effects**: `tag()`, `untag()`, `add_node()`, `update_cids()`\r\n",
      "- Modify internal state\r\n",
      "- Idempotent where sensible\r\n",
      "- Return minimal info or self for chaining\r\n",
      "\r\n",
      "### Patching: Extending Classes\r\n",
      "\r\n",
      "Use `@patch` to add methods to `HyFS` and `FSNode` after class definition. Keeps related functionality together without bloating class body.\r\n",
      "\r\n",
      "Pattern:\r\n",
      "```python\r\n",
      "@patch\r\n",
      "def method_name(self:ClassName, ...):\r\n",
      "    \"\"\"Docstring\"\"\"\r\n",
      "    # implementation\r\n",
      "```\r\n",
      "\r\n",
      "## Key Implementation Details\r\n",
      "\r\n",
      "### Index Maintenance\r\n",
      "\r\n",
      "**Three indexes** updated atomically in `add_node()`:\r\n",
      "- `path_index[path] = eid`\r\n",
      "- `children_index[parent_eid].add(eid)`\r\n",
      "- `nodes[eid] = node`\r\n",
      "\r\n",
      "When write operations added (rename/move), all three must update atomically.\r\n",
      "\r\n",
      "### Xattr Helpers\r\n",
      "\r\n",
      "Centralized xattr handling through three functions:\r\n",
      "- `_get_xattr(path, key, default)`: Safe read with fallback\r\n",
      "- `_set_xattr(path, key, value)`: Safe write, returns success boolean\r\n",
      "- `_ensure_xattr(path, key, compute_fn)`: Read-or-compute-and-store pattern\r\n",
      "\r\n",
      "All HyFS xattrs use `user.hyfs.*` namespace. Failures handled gracefully (no exceptions).\r\n",
      "\r\n",
      "### Tag Cleanup\r\n",
      "\r\n",
      "`untag()` removes empty tag sets from both `tags` and `eid_tags` to prevent `defaultdict` accumulation. Tradeoff: extra check on every untag, but keeps dict keys clean.\r\n",
      "\r\n",
      "### defaultdict Usage\r\n",
      "\r\n",
      "- `tags`: `defaultdict(set)` - tags auto-create on first use\r\n",
      "- `eid_tags`: `defaultdict(set)` - reverse index auto-creates\r\n",
      "- `children_index`: `defaultdict(set)` - children auto-create\r\n",
      "- `relations`: `defaultdict(lambda: defaultdict(set))` - two-level auto-creation\r\n",
      "\r\n",
      "Enables `hyfs.tags[new_tag].add(eid)` without checking if tag exists.\r\n",
      "\r\n",
      "## Performance Characteristics\r\n",
      "\r\n",
      "**O(1) - Fast**:\r\n",
      "- Lookup by eid: `get(eid)`\r\n",
      "- Lookup by path: `find_by_path(path)` (with index)\r\n",
      "- Get all tagged: `tagged(tag)`\r\n",
      "- Get tags of eid: `tags_of(eid)` (with bidirectional index)\r\n",
      "- Get children: `children_index[parent_eid]`\r\n",
      "\r\n",
      "**O(n) - Linear**:\r\n",
      "- Filter/find operations (scan all nodes)\r\n",
      "- Filesystem scan\r\n",
      "- Tree construction (with children index)\r\n",
      "\r\n",
      "**Cached after first access**:\r\n",
      "- Content hash: `node.cid` (node dict cache)\r\n",
      "- Content hash: xattr `user.hyfs.cid` (persistent cache)\r\n",
      "- Entity ID: xattr `user.hyfs.uuid` (persistent)\r\n",
      "- Creation time: xattr `user.hyfs.ctime` (persistent)\r\n",
      "\r\n",
      "## Edge Cases Handled\r\n",
      "\r\n",
      "- **Missing xattr support**: Falls back to deterministic hash for eid\r\n",
      "- **Empty tag sets**: Auto-cleanup in `untag()`\r\n",
      "- **Multiple roots**: `tree()` requires explicit root_path\r\n",
      "- **Non-existent paths**: `find_by_path()` returns `None`\r\n",
      "- **Duplicate tags/untags**: Idempotent operations\r\n",
      "- **Directories**: `cid` returns `None` (no content)\r\n",
      "- **Stale content hashes**: `update_cids()` forces recompute\r\n",
      "- **Missing parent in index**: `children_index` check handles gracefully\r\n",
      "\r\n",
      "## Persistence Strategy (Future)\r\n",
      "\r\n",
      "**Current state**: All data in-memory only. Tags/relations lost on session end. Xattrs persist on filesystem.\r\n",
      "\r\n",
      "**What persists now**:\r\n",
      "- `user.hyfs.uuid`: Entity ID (when xattr available)\r\n",
      "- `user.hyfs.ctime`: Creation time (when xattr available)\r\n",
      "- `user.hyfs.cid`: Content hash (when xattr available)\r\n",
      "\r\n",
      "**Planned approach**:\r\n",
      "- Xattrs for per-file metadata (eid, ctime, cid)\r\n",
      "- Separate index file for semantic layer (tags, relations, snapshots)\r\n",
      "- Format: JSON initially (human-readable), SQLite later (performance)\r\n",
      "- Index references eids, not paths (stable across moves)\r\n",
      "- Enables tracking entities that don't currently exist on filesystem\r\n",
      "\r\n",
      "**Rationale**: Tags are index metadata, not file metadata. Separation of concerns matches architecture.\r\n",
      "\r\n",
      "## Dependencies\r\n",
      "\r\n",
      "- **Python 3.12+**: For `match` statements, walrus operator\r\n",
      "- **fastcore**: `AttrDict`, `L`, `@patch`\r\n",
      "- **pathlib**: All path operations\r\n",
      "- **hashlib.sha256**: Content hashing (stdlib)\r\n",
      "- **uuid**: Entity identification (stdlib)\r\n",
      "- **os.{get,set,remove}xattr**: Extended attributes (stdlib, Unix only)\r\n",
      "- **collections.defaultdict**: Auto-creating dicts (stdlib)\r\n",
      "\r\n",
      "No external dependencies beyond fastcore.\r\n",
      "\r\n",
      "---\r\n",
      "\r\n",
      "*This document describes current implementation. Update as architecture evolves.*\r\n",
      "\r\n",
      "\r\n",
      "Key updates:\r\n",
      "- âœ… Added custom `__repr__` details for both classes\r\n",
      "- âœ… Documented `children_index` and O(n) tree construction\r\n",
      "- âœ… Documented bidirectional tag index (`eid_tags`)\r\n",
      "- âœ… Updated eid computation to show `ctime` as first-class metadata\r\n",
      "- âœ… Added two-tier caching for `cid` (xattr + node dict)\r\n",
      "- âœ… Added `update_cids()` method documentation\r\n",
      "- âœ… Updated performance characteristics to show new O(1) operations\r\n",
      "- âœ… Added xattr helpers section\r\n",
      "- âœ… Updated persistence strategy to show what persists now\r\n"
     ]
    }
   ],
   "source": [
    "%cat notes/arch.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734146f2",
   "metadata": {
    "hide_input": true,
    "time_run": "5:15:06p"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import os\n",
    "import errno\n",
    "from hashlib import sha256\n",
    "from pathlib import Path\n",
    "from fastcore.basics import AttrDict, patch\n",
    "from fastcore.foundation import L\n",
    "from fnmatch import fnmatch\n",
    "from collections import defaultdict\n",
    "\n",
    "# Xattr helpers\n",
    "def _get_xattr(path, key, default=None):\n",
    "    \"\"\"Get HyFS xattr value, return default if not found\"\"\"\n",
    "    try:\n",
    "        return os.getxattr(str(path), f'user.hyfs.{key}').decode()\n",
    "    except OSError:\n",
    "        return default\n",
    "\n",
    "def _set_xattr(path, key, value):\n",
    "    \"\"\"Set HyFS xattr value, return True on success\"\"\"\n",
    "    try:\n",
    "        os.setxattr(str(path), f'user.hyfs.{key}', str(value).encode())\n",
    "        return True\n",
    "    except OSError:\n",
    "        return False\n",
    "\n",
    "def _ensure_xattr(path, key, compute_fn):\n",
    "    \"\"\"Get xattr value, computing and storing if missing. Returns (value, stored_successfully)\"\"\"\n",
    "    value = _get_xattr(path, key)\n",
    "    if value is not None:\n",
    "        return value, True\n",
    "    \n",
    "    value = compute_fn()\n",
    "    stored = _set_xattr(path, key, value)\n",
    "    return value, stored\n",
    "\n",
    "class FSNode(AttrDict):\n",
    "    def __getattribute__(self, key):\n",
    "        cls = object.__getattribute__(self, '__class__')\n",
    "        if key in cls.__dict__ and isinstance(cls.__dict__[key], property):\n",
    "            return cls.__dict__[key].fget(self)\n",
    "        return super().__getattribute__(key)\n",
    "\n",
    "class HyFS:\n",
    "    def __init__(self):\n",
    "        self.nodes = {}  # eid -> FSNode\n",
    "        self.path_index = {}  # path -> eid\n",
    "        self.children_index = defaultdict(set)  # parent_eid -> {child_eids}\n",
    "        self.tags = defaultdict(set)  # tag_name -> {eid, ...}\n",
    "        self.eid_tags = defaultdict(set)  # eid -> {tag_name, ...}\n",
    "        self.relations = defaultdict(lambda: defaultdict(set))  # eid -> {rel_type -> {eid, ...}}\n",
    "\n",
    "    \n",
    "    def add_node(self, path, eid=None, **metadata):\n",
    "        \"\"\"Add a node to the flat storage\"\"\"\n",
    "        if eid is None:\n",
    "            eid = _compute_eid(path)\n",
    "        node = FSNode(\n",
    "            path=path,\n",
    "            eid=eid,\n",
    "            type='dir' if path.is_dir() else 'file',\n",
    "            **metadata\n",
    "        )\n",
    "        self.nodes[eid] = node\n",
    "        self.path_index[path] = eid\n",
    "        \n",
    "        # Update children index\n",
    "        parent_path = path.parent\n",
    "        parent_eid = self.path_index.get(parent_path)\n",
    "        if parent_eid:\n",
    "            self.children_index[parent_eid].add(eid)\n",
    "        \n",
    "        return eid\n",
    "\n",
    "    def get(self, eid):\n",
    "        \"\"\"O(1) lookup by eid\"\"\"\n",
    "        return self.nodes[eid]\n",
    "    \n",
    "    def find_by_path(self, path):\n",
    "        \"\"\"Find node by path (O(1) with index)\"\"\"\n",
    "        path = Path(path)\n",
    "        eid = self.path_index.get(path)\n",
    "        return self.nodes.get(eid) if eid else None\n",
    "    \n",
    "    def tree(self, root_path=None):\n",
    "        \"\"\"Build hierarchical tree view from flat storage\"\"\"\n",
    "        if root_path is None:\n",
    "            # Find root (node with no parent in our set)\n",
    "            roots = []\n",
    "            for node in self.nodes.values():\n",
    "                if not any(node.path.is_relative_to(other.path) and node.path != other.path \n",
    "                          for other in self.nodes.values()):\n",
    "                    roots.append(node)\n",
    "            if len(roots) == 1:\n",
    "                root_path = roots[0].path\n",
    "            else:\n",
    "                raise ValueError(\"Multiple roots found, specify root_path\")\n",
    "        else:\n",
    "            root_path = Path(root_path)\n",
    "        \n",
    "        root_node = self.find_by_path(root_path)\n",
    "        if not root_node:\n",
    "            raise ValueError(f\"Root path {root_path} not found in nodes\")\n",
    "        \n",
    "        return self._build_tree_node(root_node)\n",
    "    \n",
    "    def _build_tree_node(self, node):\n",
    "        \"\"\"Recursively build tree structure for a node (O(n) with children index)\"\"\"\n",
    "        tree_node = FSNode(node)  # Copy node data\n",
    "        \n",
    "        if node.type == 'dir':\n",
    "            # Use children index for O(1) lookup\n",
    "            child_eids = self.children_index.get(node.eid, set())\n",
    "            children = []\n",
    "            for child_eid in child_eids:\n",
    "                child_node = self.nodes[child_eid]\n",
    "                children.append(self._build_tree_node(child_node))\n",
    "            tree_node['children'] = children\n",
    "        \n",
    "        return tree_node\n",
    "\n",
    "    def filter(self, pred):\n",
    "        \"\"\"Filter nodes by predicate, returns flat list\"\"\"\n",
    "        return L([node for node in self.nodes.values() if pred(node)])\n",
    "    \n",
    "    def find(self, pattern):\n",
    "        \"\"\"Find nodes matching glob pattern\"\"\"\n",
    "        return self.filter(lambda n: fnmatch(n.path.name, pattern))\n",
    "\n",
    "    def __repr__(self):\n",
    "        n_files = sum(1 for n in self.nodes.values() if n.type == 'file')\n",
    "        n_dirs = sum(1 for n in self.nodes.values() if n.type == 'dir')\n",
    "        n_tags = len(self.tags)\n",
    "        return f\"HyFS(ğŸ“„ {n_files} files, ğŸ“ {n_dirs} dirs, ğŸ·ï¸  {n_tags} tags)\"\n",
    "\n",
    "def _compute_eid(path):\n",
    "    \"\"\"Compute stable UUID for a path. Uses xattr if available, else deterministic hash from creation time.\"\"\"\n",
    "    # Always try to ensure ctime is stored (valuable metadata)\n",
    "    ctime, _ = _ensure_xattr(path, 'ctime', lambda: str(path.stat().st_mtime))\n",
    "    \n",
    "    # Try to get existing UUID\n",
    "    eid = _get_xattr(path, 'uuid')\n",
    "    if eid:\n",
    "        return eid\n",
    "    \n",
    "    # No UUID yet, generate one\n",
    "    new_uuid = str(uuid.uuid4())\n",
    "    \n",
    "    # Try to store it\n",
    "    if _set_xattr(path, 'uuid', new_uuid):\n",
    "        return new_uuid\n",
    "    \n",
    "    # Xattr not supported for UUID, fall back to deterministic hash\n",
    "    # Use ctime (from xattr if available, else st_mtime from above)\n",
    "    s = path.stat()\n",
    "    data = f\"{s.st_dev}:{s.st_ino}:{ctime}\".encode()\n",
    "    hash_hex = sha256(data).hexdigest()\n",
    "    return f\"{hash_hex[:8]}-{hash_hex[8:12]}-{hash_hex[12:16]}-{hash_hex[16:20]}-{hash_hex[20:32]}\"\n",
    "\n",
    "def _compute_cid(path):\n",
    "    \"\"\"Compute SHA256 content hash for a file. Uses xattr cache if available.\"\"\"\n",
    "    if not path.is_file():\n",
    "        return None\n",
    "    \n",
    "    # Check for cached cid in xattr\n",
    "    cached_cid = _get_xattr(path, 'cid')\n",
    "    if cached_cid:\n",
    "        return cached_cid\n",
    "    \n",
    "    # Compute hash\n",
    "    h = sha256()\n",
    "    with open(path, 'rb') as f:\n",
    "        while chunk := f.read(65536):  # 64KB chunks\n",
    "            h.update(chunk)\n",
    "    \n",
    "    cid = h.hexdigest()\n",
    "    \n",
    "    # Try to cache it\n",
    "    _set_xattr(path, 'cid', cid)\n",
    "    \n",
    "    return cid\n",
    "\n",
    "def scan_fs(root_path, include_metadata=False):\n",
    "    \"\"\"Scan filesystem and populate HyFS flat storage\"\"\"\n",
    "    hyfs = HyFS()\n",
    "    root_path = Path(root_path)\n",
    "    \n",
    "    # Walk the entire tree\n",
    "    for path in [root_path] + list(root_path.rglob('*')):\n",
    "        metadata = {}\n",
    "        if include_metadata:\n",
    "            # Add any metadata you want here\n",
    "            pass\n",
    "        hyfs.add_node(path, **metadata)\n",
    "    \n",
    "    return hyfs\n",
    "\n",
    "# Add cid property to FSNode\n",
    "@property\n",
    "def cid(self):\n",
    "    \"\"\"Lazy-computed content ID (SHA256 hash) for files\"\"\"\n",
    "    if 'cid' not in self:\n",
    "        self['cid'] = _compute_cid(self.path)\n",
    "    return self['cid']\n",
    "\n",
    "FSNode.cid = cid\n",
    "\n",
    "@patch\n",
    "def show(self:FSNode, indent=0):\n",
    "    \"\"\"Display tree node (works on tree view)\"\"\"\n",
    "    print('    ' * indent + self.path.name)\n",
    "    if 'children' in self:\n",
    "        for child in self.children:\n",
    "            child.show(indent+1)\n",
    "\n",
    "@patch\n",
    "def filter(self:FSNode, pred):\n",
    "    \"\"\"Filter tree node recursively (works on tree view)\"\"\"\n",
    "    matches = L()\n",
    "    if pred(self): matches.append(self)\n",
    "    if 'children' in self:\n",
    "        for child in self.children:\n",
    "            matches += child.filter(pred)\n",
    "    return matches\n",
    "\n",
    "@patch\n",
    "def find(self:FSNode, pattern):\n",
    "    \"\"\"Find in tree node (works on tree view)\"\"\"\n",
    "    return self.filter(lambda n: fnmatch(n.path.name, pattern))\n",
    "\n",
    "@patch\n",
    "def __repr__(self:FSNode):\n",
    "    name = self.path.name if hasattr(self, 'path') else 'unknown'\n",
    "    type_icon = 'ğŸ“' if self.get('type') == 'dir' else 'ğŸ“„'\n",
    "    eid_short = self.eid[:8] if hasattr(self, 'eid') else 'no-eid'\n",
    "    return f\"FSNode({type_icon} {name!r}, {eid_short}...)\"\n",
    "\n",
    "@patch\n",
    "def tag(self:HyFS, eid, tag):\n",
    "    \"\"\"Add a tag to an eid (idempotent)\"\"\"\n",
    "    self.tags[tag].add(eid)\n",
    "    self.eid_tags[eid].add(tag)\n",
    "\n",
    "@patch\n",
    "def untag(self:HyFS, eid, tag):\n",
    "    \"\"\"Remove a tag from an eid (idempotent)\"\"\"\n",
    "    self.tags[tag].discard(eid)\n",
    "    self.eid_tags[eid].discard(tag)\n",
    "    \n",
    "    # Clean up empty sets\n",
    "    if not self.tags[tag]:\n",
    "        del self.tags[tag]\n",
    "    if not self.eid_tags[eid]:\n",
    "        del self.eid_tags[eid]\n",
    "\n",
    "@patch\n",
    "def tagged(self:HyFS, tag):\n",
    "    \"\"\"Get all eids with this tag\"\"\"\n",
    "    return self.tags[tag]  # Returns set (possibly empty)\n",
    "\n",
    "@patch\n",
    "def tags_of(self:HyFS, eid):\n",
    "    \"\"\"Get all tags for this eid (O(1) with bidirectional index)\"\"\"\n",
    "    return self.eid_tags[eid]  # Returns set (possibly empty)\n",
    "\n",
    "@patch\n",
    "def update_cids(self:L):\n",
    "    \"\"\"Update cids for a list of nodes (chainable), recomputing from disk\"\"\"\n",
    "    for node in self:\n",
    "        if node.type == 'file':\n",
    "            # Clear node dict cache\n",
    "            if 'cid' in node:\n",
    "                del node['cid']\n",
    "            # Clear xattr cache to force recompute\n",
    "            try:\n",
    "                os.removexattr(str(node.path), 'user.hyfs.cid')\n",
    "            except OSError:\n",
    "                pass  # Wasn't set or xattr not supported\n",
    "            # Now access will recompute from file content\n",
    "            _ = node.cid\n",
    "    return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6eaebb",
   "metadata": {
    "hide_input": true,
    "time_run": "5:15:06p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Scanning and repr ===\n",
      "HyFS(ğŸ“„ 60 files, ğŸ“ 36 dirs, ğŸ·ï¸  0 tags)\n",
      "\n",
      "=== Test 2: FSNode repr ===\n",
      "FSNode(ğŸ“„ 'README.md', 9a6f0b1c...)\n",
      "FSNode(ğŸ“ 'config', 850b8fb2...)\n",
      "\n",
      "=== Test 3: Find returns L with good repr ===\n",
      "Found: [FSNode(ğŸ“„ 'utils.py', 714e2121...), FSNode(ğŸ“„ '__init__.py', a3658657...), FSNode(ğŸ“„ 'main.py', 84392f69...), FSNode(ğŸ“„ 'test_main.py', ba023b1c...), FSNode(ğŸ“„ '__init__.py', a7ce510b...), FSNode(ğŸ“„ 'product.py', 28dfa4b7...), FSNode(ğŸ“„ '__init__.py', d665e189...), FSNode(ğŸ“„ 'user.py', aab69b19...)]\n",
      "First 3: [FSNode(ğŸ“„ 'utils.py', 714e2121...), FSNode(ğŸ“„ '__init__.py', a3658657...), FSNode(ğŸ“„ 'main.py', 84392f69...)]\n",
      "\n",
      "=== Test 4: Bidirectional tagging ===\n",
      "Tags for utils.py: {'important', 'code'}\n",
      "Tagged 'important': 3 files\n",
      "\n",
      "=== Test 5: Tag cleanup ===\n",
      "After untag: {'code'}\n",
      "All tags in system: ['important', 'code']\n",
      "After removing all tags from first file: set()\n",
      "eid_tags has entry? True\n",
      "\n",
      "=== Test 6: Tree construction ===\n",
      "Tree built in 5.80ms\n",
      "Root: FSNode(ğŸ“ 'fs', 1907aacb...)\n",
      "Root has 33 children\n",
      "\n",
      "=== Test 7: Content hashing ===\n",
      "Computing cid for README.md...\n",
      "CID: d4ad9c87725fe5e6...\n",
      "Cached in xattr? True\n",
      "Cached value matches? True\n",
      "Second access returns same? True\n",
      "\n",
      "=== Test 8: update_cids chainable ===\n",
      "Original CID: bf573149b23303ca...\n",
      "New CID: 4ccfac83d4aadc93...\n",
      "CID changed? True\n",
      "\n",
      "Chaining: hyfs.find('*.txt').update_cids()\n",
      "Returns L? L\n",
      "Length: 10\n",
      "\n",
      "=== Test 9: EID stability ===\n",
      "First eid: 3804e56f...\n",
      "Stored in xattr? True\n",
      "Second eid: 3804e56f...\n",
      "Stable? True\n",
      "\n",
      "=== Test 10: Creation time xattr ===\n",
      "UUID stored in xattr? True\n",
      "Creation time stored? True\n",
      "Creation time: 2025-10-27 14:53:01\n",
      "\n",
      "=== Test 11: Children index ===\n",
      "src/ has 4 children in index\n",
      "Actual children from path check: 4\n",
      "Index correct? True\n",
      "\n",
      "=== Test 12: Tree view filtering ===\n",
      "Found 8 .py files in tree view\n",
      "First 3: [FSNode(ğŸ“„ '__init__.py', a3658657...), FSNode(ğŸ“„ 'utils.py', 714e2121...), FSNode(ğŸ“„ 'main.py', 84392f69...)]\n",
      "\n",
      "âœ… All tests complete!\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Basic scanning and repr\n",
    "print(\"=== Test 1: Scanning and repr ===\")\n",
    "hyfs = scan_fs('/app/data/dev/hyfs/test/fs')\n",
    "print(hyfs)\n",
    "print()\n",
    "\n",
    "# Test 2: FSNode repr\n",
    "print(\"=== Test 2: FSNode repr ===\")\n",
    "readme = hyfs.find_by_path(root / 'README.md')\n",
    "print(readme)\n",
    "config_dir = hyfs.find_by_path(root / 'config')\n",
    "print(config_dir)\n",
    "print()\n",
    "\n",
    "# Test 3: Find and filter with new repr\n",
    "print(\"=== Test 3: Find returns L with good repr ===\")\n",
    "py_files = hyfs.find('*.py')\n",
    "print(f\"Found: {py_files}\")\n",
    "print(f\"First 3: {py_files[:3]}\")\n",
    "print()\n",
    "\n",
    "# Test 4: Bidirectional tagging (O(1) tags_of)\n",
    "print(\"=== Test 4: Bidirectional tagging ===\")\n",
    "# Tag some files\n",
    "for node in py_files[:3]:\n",
    "    hyfs.tag(node.eid, 'important')\n",
    "    hyfs.tag(node.eid, 'code')\n",
    "\n",
    "# O(1) lookup of tags for an eid\n",
    "first_py = py_files[0]\n",
    "print(f\"Tags for {first_py.path.name}: {hyfs.tags_of(first_py.eid)}\")\n",
    "print(f\"Tagged 'important': {len(hyfs.tagged('important'))} files\")\n",
    "print()\n",
    "\n",
    "# Test 5: Tag cleanup\n",
    "print(\"=== Test 5: Tag cleanup ===\")\n",
    "hyfs.untag(first_py.eid, 'important')\n",
    "print(f\"After untag: {hyfs.tags_of(first_py.eid)}\")\n",
    "print(f\"All tags in system: {list(hyfs.tags.keys())}\")\n",
    "hyfs.untag(first_py.eid, 'code')\n",
    "print(f\"After removing all tags from first file: {hyfs.tags_of(first_py.eid)}\")\n",
    "print(f\"eid_tags has entry? {first_py.eid in hyfs.eid_tags}\")  # Should be False (cleaned up)\n",
    "print()\n",
    "\n",
    "# Test 6: Tree construction with children index (O(n))\n",
    "print(\"=== Test 6: Tree construction ===\")\n",
    "import time\n",
    "start = time.time()\n",
    "tree = hyfs.tree()\n",
    "elapsed = time.time() - start\n",
    "print(f\"Tree built in {elapsed*1000:.2f}ms\")\n",
    "print(f\"Root: {tree}\")\n",
    "print(f\"Root has {len(tree.children)} children\")\n",
    "print()\n",
    "\n",
    "# Test 7: Content hashing with xattr cache\n",
    "print(\"=== Test 7: Content hashing ===\")\n",
    "readme = hyfs.find_by_path(root / 'README.md')\n",
    "print(f\"Computing cid for {readme.path.name}...\")\n",
    "cid1 = readme.cid\n",
    "print(f\"CID: {cid1[:16]}...\")\n",
    "\n",
    "# Check it's cached in xattr\n",
    "cached = _get_xattr(readme.path, 'cid')\n",
    "print(f\"Cached in xattr? {cached is not None}\")\n",
    "print(f\"Cached value matches? {cached == cid1}\")\n",
    "\n",
    "# Access again (should be instant from cache)\n",
    "cid2 = readme.cid\n",
    "print(f\"Second access returns same? {cid1 == cid2}\")\n",
    "print()\n",
    "\n",
    "# Test 8: update_cids chainable method\n",
    "print(\"=== Test 8: update_cids chainable ===\")\n",
    "# Modify a file\n",
    "test_file = root / 'test_modified.txt'\n",
    "test_file.write_text(\"original content\")\n",
    "hyfs.add_node(test_file)\n",
    "node = hyfs.find_by_path(test_file)\n",
    "original_cid = node.cid\n",
    "print(f\"Original CID: {original_cid[:16]}...\")\n",
    "\n",
    "# Modify the file\n",
    "test_file.write_text(\"modified content\")\n",
    "\n",
    "# Update cid (should recompute)\n",
    "L([node]).update_cids()\n",
    "new_cid = node.cid\n",
    "print(f\"New CID: {new_cid[:16]}...\")\n",
    "print(f\"CID changed? {original_cid != new_cid}\")\n",
    "\n",
    "# Chain with find\n",
    "print(\"\\nChaining: hyfs.find('*.txt').update_cids()\")\n",
    "result = hyfs.find('*.txt').update_cids()\n",
    "print(f\"Returns L? {type(result).__name__}\")\n",
    "print(f\"Length: {len(result)}\")\n",
    "print()\n",
    "\n",
    "# Test 9: EID stability with xattr\n",
    "print(\"=== Test 9: EID stability ===\")\n",
    "test_file2 = root / 'test_eid.txt'\n",
    "test_file2.write_text(\"test\")\n",
    "eid1 = _compute_eid(test_file2)\n",
    "print(f\"First eid: {eid1[:8]}...\")\n",
    "\n",
    "# Check it's stored in xattr\n",
    "stored_eid = _get_xattr(test_file2, 'uuid')\n",
    "print(f\"Stored in xattr? {stored_eid is not None}\")\n",
    "\n",
    "# Compute again (should return same)\n",
    "eid2 = _compute_eid(test_file2)\n",
    "print(f\"Second eid: {eid2[:8]}...\")\n",
    "print(f\"Stable? {eid1 == eid2}\")\n",
    "print()\n",
    "\n",
    "# Test 10: Creation time xattr (always stored)\n",
    "print(\"=== Test 10: Creation time xattr ===\")\n",
    "# ctime should always be stored as valuable metadata\n",
    "ctime = _get_xattr(readme.path, 'ctime')\n",
    "uuid_stored = _get_xattr(readme.path, 'uuid')\n",
    "print(f\"UUID stored in xattr? {uuid_stored is not None}\")\n",
    "print(f\"Creation time stored? {ctime is not None}\")\n",
    "if ctime:\n",
    "    import datetime\n",
    "    timestamp = float(ctime)\n",
    "    dt = datetime.datetime.fromtimestamp(timestamp)\n",
    "    print(f\"Creation time: {dt.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print()\n",
    "\n",
    "# Test 11: Children index correctness\n",
    "print(\"=== Test 11: Children index ===\")\n",
    "src_dir = hyfs.find_by_path(root / 'src')\n",
    "if src_dir:\n",
    "    children_eids = hyfs.children_index[src_dir.eid]\n",
    "    print(f\"src/ has {len(children_eids)} children in index\")\n",
    "    # Verify by checking actual children\n",
    "    actual_children = [n for n in hyfs.nodes.values() if n.path.parent == src_dir.path]\n",
    "    print(f\"Actual children from path check: {len(actual_children)}\")\n",
    "    print(f\"Index correct? {len(children_eids) == len(actual_children)}\")\n",
    "print()\n",
    "\n",
    "# Test 12: Filter on tree view\n",
    "print(\"=== Test 12: Tree view filtering ===\")\n",
    "tree = hyfs.tree()\n",
    "py_in_tree = tree.find('*.py')\n",
    "print(f\"Found {len(py_in_tree)} .py files in tree view\")\n",
    "print(f\"First 3: {py_in_tree[:3]}\")\n",
    "print()\n",
    "# Clean up test files\n",
    "test_file.unlink(missing_ok=True)\n",
    "test_file2.unlink(missing_ok=True)\n",
    "\n",
    "print(\"âœ… All tests complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef8b443",
   "metadata": {
    "collapsed": true,
    "skipped": true,
    "solveit_ai": true
   },
   "source": [
    "OK, from here to a MVP / PoC, what are the obvious missing features?\n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Looking at your HyFS implementation, here are the obvious missing features to reach a solid MVP/PoC:\n",
    "\n",
    "**1. Relations API** - You have the data structure (`self.relations`) but no methods to use it:\n",
    "```python\n",
    "relate(eid1, rel_type, eid2)  # e.g., 'imports', 'generates', 'depends_on'\n",
    "unrelate(eid1, rel_type, eid2)\n",
    "related(eid, rel_type)  # get all eids related via this type\n",
    "relations_of(eid)  # get all relations for this eid\n",
    "```\n",
    "\n",
    "**2. Persistence** - Everything's in-memory only. At minimum:\n",
    "```python\n",
    "save(path)  # serialize semantic layer (tags, relations)\n",
    "load(path)  # restore from saved state\n",
    "```\n",
    "\n",
    "**3. Filtered tree views** - Build trees from subsets:\n",
    "```python\n",
    "tree_from(eids, root_eid=None)  # tree view of just these nodes\n",
    "tagged_tree(tag)  # tree view of tagged subset\n",
    "```\n",
    "\n",
    "**4. Duplicate detection** - You have `cid`, use it:\n",
    "```python\n",
    "duplicates()  # find files with same cid\n",
    "```\n",
    "\n",
    "**5. Basic stats/queries**:\n",
    "```python\n",
    "stats()  # file count by extension, size distribution, etc.\n",
    "largest(n=10)  # biggest files\n",
    "newest(n=10)  # most recently modified\n",
    "```\n",
    "\n",
    "**6. Path operations as Plans** (your design doc mentions this):\n",
    "```python\n",
    "rename_plan(eid, new_name)  # returns Plan object\n",
    "move_plan(eid, new_parent)  # returns Plan object\n",
    "# plan.preview() then plan.execute()\n",
    "```\n",
    "\n",
    "Which of these interests you most? I'd suggest starting with **relations** (completes your semantic layer) or **persistence** (makes it actually useful beyond one session)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b242f8e",
   "metadata": {
    "collapsed": true,
    "skipped": true,
    "solveit_ai": true,
    "use_thinking": true
   },
   "source": [
    "OK, from here to a MVP / PoC, what are the obvious missing features?\n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ ğŸ§ \n",
    "\n",
    "Looking at your current implementation, here are the obvious gaps to reach MVP/PoC:\n",
    "\n",
    "**Critical for MVP:**\n",
    "\n",
    "1. **Persistence of semantic layer** - Tags/relations disappear on session end. Need to serialize to disk (JSON initially per your design doc). The xattrs persist but the tags don't.\n",
    "\n",
    "2. **Write operations** - You can read/analyze but not act:\n",
    "   - `rename(eid, new_name)` \n",
    "   - `move(eid, new_parent_path)`\n",
    "   - `delete(eid)`\n",
    "   - All should update the three indexes atomically\n",
    "   - Consider the Plan/Transaction pattern mentioned in design doc for safety\n",
    "\n",
    "3. **Load from persisted state** - `HyFS.load(path)` to restore a saved session. Pair with `save()`.\n",
    "\n",
    "**High value for demonstrating the concept:**\n",
    "\n",
    "4. **Duplicate detection** - You have `cid` but no API:\n",
    "   - `duplicates()` â†’ groups files by content hash\n",
    "   - `deduplicate(strategy='keep-first')` â†’ returns a plan\n",
    "\n",
    "5. **Relations API** - Mentioned in design but not implemented:\n",
    "   - `relate(eid1, rel_type, eid2)` - e.g., \"imports\", \"generates\", \"depends-on\"\n",
    "   - `related(eid, rel_type)` â†’ get related entities\n",
    "   - Would demonstrate graph capabilities beyond just tags\n",
    "\n",
    "6. **Filtered tree views** - Build trees from tagged subsets:\n",
    "   - `tree(filter=lambda n: 'important' in hyfs.tags_of(n.eid))`\n",
    "   - Shows multiple views of same data\n",
    "\n",
    "**Nice to have but not essential:**\n",
    "\n",
    "7. **Snapshot/diff** - Compare filesystem states over time\n",
    "8. **Basic CLI** - `hyfs tag`, `hyfs find`, etc. (makes it feel like a tool)\n",
    "9. **More metadata properties** - `size`, `mtime` as properties on FSNode\n",
    "\n",
    "**What would make a compelling demo?**\n",
    "\n",
    "A script that:\n",
    "1. Scans a project directory\n",
    "2. Tags files by category\n",
    "3. Finds duplicates\n",
    "4. Saves the state\n",
    "5. Later loads it and shows tags persisted despite file renames\n",
    "\n",
    "The killer features are **stable identity through changes** + **semantic organization** + **content awareness**. You need persistence and write operations to actually demonstrate those."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d232a45b",
   "metadata": {},
   "source": [
    "Here are thoughts by you in previous messages about obvious gaps to reach a solid MVP/PoC, based on what we had discussed.\n",
    "\n",
    "### Critical\n",
    "\n",
    "1. **Persistence of semantic layer** - Tags/relations disappear on session end. Need to serialize to disk (JSON initially per your design doc). The xattrs persist but the tags don't. At minimum:\n",
    "\n",
    "    ```python\n",
    "    HyFS.save(path)  # serialize semantic layer (eids, cids, Path, tags, relations...)\n",
    "    HyFS.load(path)  # restore from saved state\n",
    "    ```\n",
    "\n",
    "2. **Write operations** - we can read/analyze but not act:\n",
    "   - `rename(eid, new_name)` \n",
    "   - `move(eid, new_parent_path)`\n",
    "   - `delete(eid)`\n",
    "   - All should update the three indexes atomically\n",
    "   - Consider the Plan/Transaction pattern mentioned in design doc for safety\n",
    "\n",
    "### High value for demonstrating the concept\n",
    "\n",
    "3. **Filtered tree views** - Build trees from tagged subsets:\n",
    "   - `tree(filter=lambda n: 'important' in hyfs.tags_of(n.eid))`\n",
    "   - Shows multiple views of same data\n",
    "\n",
    "4. **Relations API** - Mentioned in design but not implemented:\n",
    "   - `relate(eid1, rel_type, eid2)` - e.g., \"imports\", \"generates\", \"depends-on\"\n",
    "   - `related(eid, rel_type)` â†’ get related entities\n",
    "   - Would demonstrate graph capabilities beyond just tags\n",
    "\n",
    "### Eventually required\n",
    "\n",
    "5. **More metadata properties** - `size`, `mtime` as properties on FSNode\n",
    "\n",
    "6. **Duplicate detection** - we have `cid`, use it:\n",
    "\n",
    "    ```python\n",
    "    duplicates()  # find files with same cid\n",
    "    deduplicate(strategy='keep-first')  # deduplicate files, possibly with a \"Plan\" later on (TBD, first just the low-level method)\n",
    "    ```\n",
    "\n",
    "### Nice to have but not essential\n",
    "\n",
    "7. **Basic CLI** - `hyfs tag`, `hyfs find`, etc. (makes it feel like a tool)\n",
    "\n",
    "8. **Basic stats/queries**:\n",
    "\n",
    "    ```python\n",
    "    stats()  # file count by extension, size distribution, etc.\n",
    "    largest(n=10)  # biggest files\n",
    "    newest(n=10)  # most recently modified\n",
    "    ```\n",
    "\n",
    "9. **Path operations as Plans** - similar to dry run. The reason why I'm not too big on it for now is that I'd rather have those base ops first (see 2. above) before wrapping them in a Plan. But it's a good idea for safety, we'll see. I remember working with opinionated libraries (something about graphs or whatever, don't remember), and it's not always great if not done very well.\n",
    "\n",
    "    ```python\n",
    "    rename_plan(eid, new_name)  # returns Plan object\n",
    "    move_plan(eid, new_parent)  # returns Plan object\n",
    "    # plan.preview() then plan.execute()\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b90f81f",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Thoughts?\n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Looking at your gap analysis, I think you've got the priorities exactly right. Here are my thoughts:\n",
    "\n",
    "**Your instincts are spot-on:**\n",
    "\n",
    "1. **Persistence is critical** - Without it, HyFS is just an expensive `find` command. The whole point of stable eids and semantic organization evaporates when the session ends. The xattr approach for per-file metadata (uuid, cid, ctime) is solid, but tags/relations need their own persistence layer. JSON initially makes senseâ€”human-readable, easy to debug, no deps.\n",
    "\n",
    "2. **Write operations before Plans** - Totally agree. Build the atomic operations first (`rename`, `move`, `delete` that update all three indexes), *then* wrap them in Plans if needed. Starting with Plans is premature abstraction. Get the mechanics right, then add safety rails.\n",
    "\n",
    "3. **Filtered tree views are the killer demo** - This is where HyFS really shines vs regular filesystem tools. \"Show me a tree of just the files tagged 'important'\" or \"tree view of everything related to this module\" demonstrates the multiple-views-of-same-data concept beautifully.\n",
    "\n",
    "**Sequencing suggestion:**\n",
    "\n",
    "Start with **#1 (persistence)** and **#6 (duplicates)** together. Why? Because:\n",
    "- Persistence gives you a real use case to test against (save, modify filesystem, reload)\n",
    "- Duplicates are trivial to implement (already have `cid`) and immediately useful\n",
    "- Together they make a compelling story: \"Find duplicates, tag them, save your work, come back later\"\n",
    "\n",
    "Then **#2 (write ops)**, because you need them to act on what you've discovered.\n",
    "\n",
    "Then **#3 (filtered trees)** to demonstrate the power.\n",
    "\n",
    "**Skip for now:**\n",
    "- Relations API (#4) - tags are simpler and cover 80% of use cases. Add relations when you hit a real need that tags can't handle.\n",
    "- CLI (#7) - REPL-first is the right approach. CLI can come later when the API is stable.\n",
    "- Plans (#9) - as you said, base ops first.\n",
    "\n",
    "**Quick win:**\n",
    "#8 (stats) is trivially easy and makes exploration more satisfying. A simple `hyfs.stats()` that shows file counts by extension, total size, etc. is like 10 lines of code and makes the tool feel more complete.\n",
    "\n",
    "**One addition:**\n",
    "Consider a `refresh()` method that rescans the filesystem and updates nodes that changed (new files, deleted files, modified content). Useful for long-running sessions. Could be as simple as \"rescan and merge\" initially.\n",
    "\n",
    "Your architecture is solid. The flat storage + multiple indexes design will scale well. The xattr approach for per-file metadata is the right call. The fastcore idioms make it feel native to the ecosystem.\n",
    "\n",
    "**Bottom line**: Persistence first, then write ops, then filtered trees. That's your MVP. Everything else is polish."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83bd868",
   "metadata": {
    "collapsed": true,
    "solveit_ai": true
   },
   "source": [
    "Strongly agreed!\n",
    "\n",
    "If you agree, let's proceed with #1 (persistence) and #6 (duplicates) together. Let's patch the `save` and `load` methods in `HyFS` to handle the serialization of the semantic layer. We'll use JSON for simplicity now, knowing it's already a good fit for a later SQLite implementation.\n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Perfect! Let's build persistence and duplicates together. Starting with the save/load methods for the semantic layer, then adding duplicate detection.\n",
    "\n",
    "```python\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "@patch\n",
    "def save(self:HyFS, path):\n",
    "    \"\"\"Save semantic layer (tags, relations, metadata) to JSON\"\"\"\n",
    "    path = Path(path)\n",
    "    \n",
    "    # Build the semantic layer structure\n",
    "    data = {\n",
    "        'version': '0.1.0',\n",
    "        'saved_at': datetime.now().isoformat(),\n",
    "        'nodes': {},\n",
    "        'tags': {},\n",
    "        'relations': {}\n",
    "    }\n",
    "    \n",
    "    # Save node metadata (eid, path, cid, type)\n",
    "    # We store paths as strings so we can track entities even if they move/disappear\n",
    "    for eid, node in self.nodes.items():\n",
    "        data['nodes'][eid] = {\n",
    "            'path': str(node.path),\n",
    "            'type': node.type,\n",
    "            'cid': node.get('cid'),  # May be None for dirs or uncached\n",
    "        }\n",
    "    \n",
    "    # Save tags (convert sets to lists for JSON)\n",
    "    for tag, eids in self.tags.items():\n",
    "        data['tags'][tag] = list(eids)\n",
    "    \n",
    "    # Save relations (nested defaultdict -> nested dict)\n",
    "    for eid, rel_dict in self.relations.items():\n",
    "        data['relations'][eid] = {\n",
    "            rel_type: list(target_eids) \n",
    "            for rel_type, target_eids in rel_dict.items()\n",
    "        }\n",
    "    \n",
    "    # Write atomically (write to temp, then rename)\n",
    "    temp_path = path.with_suffix('.tmp')\n",
    "    temp_path.write_text(json.dumps(data, indent=2))\n",
    "    temp_path.rename(path)\n",
    "    \n",
    "    return path\n",
    "\n",
    "@patch\n",
    "def load(self:HyFS, path):\n",
    "    \"\"\"Load semantic layer from JSON, merging with current state\"\"\"\n",
    "    path = Path(path)\n",
    "    data = json.loads(path.read_text())\n",
    "    \n",
    "    # Validate version (simple check for now)\n",
    "    if not data.get('version', '').startswith('0.1'):\n",
    "        print(f\"âš ï¸  Warning: Loading data from version {data.get('version')}, may be incompatible\")\n",
    "    \n",
    "    # Load nodes - create FSNode for each eid if path still exists\n",
    "    for eid, node_data in data['nodes'].items():\n",
    "        node_path = Path(node_data['path'])\n",
    "        if node_path.exists():\n",
    "            # Path exists, add/update node\n",
    "            # Use stored eid to maintain identity\n",
    "            self.add_node(node_path, eid=eid)\n",
    "            # Restore cached cid if available\n",
    "            if node_data.get('cid'):\n",
    "                self.nodes[eid]['cid'] = node_data['cid']\n",
    "        else:\n",
    "            # Path doesn't exist anymore - still track the eid for tag/relation history\n",
    "            # Create a \"phantom\" node\n",
    "            self.nodes[eid] = FSNode(\n",
    "                path=node_path,\n",
    "                eid=eid,\n",
    "                type=node_data['type'],\n",
    "                phantom=True  # Mark as not currently on filesystem\n",
    "            )\n",
    "            if node_data.get('cid'):\n",
    "                self.nodes[eid]['cid'] = node_data['cid']\n",
    "    \n",
    "    # Load tags (convert lists back to sets)\n",
    "    for tag, eids in data['tags'].items():\n",
    "        self.tags[tag] = set(eids)\n",
    "        # Rebuild bidirectional index\n",
    "        for eid in eids:\n",
    "            self.eid_tags[eid].add(tag)\n",
    "    \n",
    "    # Load relations (nested lists back to nested defaultdict of sets)\n",
    "    for eid, rel_dict in data['relations'].items():\n",
    "        for rel_type, target_eids in rel_dict.items():\n",
    "            self.relations[eid][rel_type] = set(target_eids)\n",
    "    \n",
    "    return self\n",
    "\n",
    "@patch\n",
    "def duplicates(self:HyFS):\n",
    "    \"\"\"Find files with duplicate content (same cid)\"\"\"\n",
    "    # Group files by cid\n",
    "    cid_groups = defaultdict(list)\n",
    "    for node in self.nodes.values():\n",
    "        if node.type == 'file' and node.cid:\n",
    "            cid_groups[node.cid].append(node)\n",
    "    \n",
    "    # Return only groups with duplicates (2+ files)\n",
    "    dupes = {cid: L(nodes) for cid, nodes in cid_groups.items() if len(nodes) > 1}\n",
    "    return dupes\n",
    "\n",
    "@patch\n",
    "def deduplicate(self:HyFS, strategy='keep-first', dry_run=True):\n",
    "    \"\"\"Remove duplicate files. Returns list of files that would be/were deleted.\n",
    "    \n",
    "    Args:\n",
    "        strategy: 'keep-first' (keep first by path), 'keep-shortest' (keep shortest path)\n",
    "        dry_run: If True, just return what would be deleted without deleting\n",
    "    \"\"\"\n",
    "    dupes = self.duplicates()\n",
    "    to_delete = []\n",
    "    \n",
    "    for cid, nodes in dupes.items():\n",
    "        # Sort by path for deterministic behavior\n",
    "        sorted_nodes = sorted(nodes, key=lambda n: str(n.path))\n",
    "        \n",
    "        if strategy == 'keep-first':\n",
    "            keep = sorted_nodes[0]\n",
    "            delete = sorted_nodes[1:]\n",
    "        elif strategy == 'keep-shortest':\n",
    "            keep = min(sorted_nodes, key=lambda n: len(str(n.path)))\n",
    "            delete = [n for n in sorted_nodes if n != keep]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "        \n",
    "        to_delete.extend(delete)\n",
    "    \n",
    "    if not dry_run:\n",
    "        for node in to_delete:\n",
    "            node.path.unlink()\n",
    "            # Remove from indexes\n",
    "            del self.nodes[node.eid]\n",
    "            del self.path_index[node.path]\n",
    "            # Clean up children index\n",
    "            parent_eid = self.path_index.get(node.path.parent)\n",
    "            if parent_eid:\n",
    "                self.children_index[parent_eid].discard(node.eid)\n",
    "    \n",
    "    return L(to_delete)\n",
    "```\n",
    "\n",
    "This gives you:\n",
    "\n",
    "**Persistence:**\n",
    "- `hyfs.save('myfs.json')` - saves tags, relations, node metadata\n",
    "- `hyfs.load('myfs.json')` - restores semantic layer, handles moved/deleted files\n",
    "- Atomic writes (temp file + rename)\n",
    "- Tracks \"phantom\" nodes (eids that existed but path is gone)\n",
    "- Version field for future migration\n",
    "\n",
    "**Duplicates:**\n",
    "- `hyfs.duplicates()` - returns dict of `{cid: [nodes]}` for all duplicate groups\n",
    "- `hyfs.deduplicate(strategy='keep-first', dry_run=True)` - removes duplicates\n",
    "- Safe by default (dry_run=True)\n",
    "- Two strategies: keep first alphabetically or keep shortest path\n",
    "\n",
    "The JSON structure is SQLite-ready - just need to map nodes/tags/relations to tables later.\n",
    "\n",
    "Want to test it on your dummy filesystem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c9e84",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Strongly agreed!\n",
    "\n",
    "If you agree, let's proceed with #1 (persistence) and #6 (duplicates) together. Let's patch the `save` and `load` methods in `HyFS` to handle the serialization of the semantic layer. We'll use JSON for simplicity now, knowing it's already a good fit for a later SQLite implementation.\n",
    "\n",
    "As usual, follow fast.ai and fastcore coding style, favoring concise vertical space and longer one-liners whenever possible in Python.\n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Let's start with persistence. We'll serialize the semantic layer (tags, relations) and enough info to reconnect to the filesystem nodes:\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "@patch\n",
    "def save(self:HyFS, path):\n",
    "    \"Save semantic layer to JSON\"\n",
    "    path = Path(path)\n",
    "    data = {\n",
    "        'tags': {tag: list(eids) for tag, eids in self.tags.items()},\n",
    "        'relations': {eid: {rel: list(targets) for rel, targets in rels.items()} \n",
    "                     for eid, rels in self.relations.items()},\n",
    "        'nodes': {eid: {'path': str(node.path), 'eid': node.eid, 'type': node.type} \n",
    "                 for eid, node in self.nodes.items()}\n",
    "    }\n",
    "    path.write_text(json.dumps(data, indent=2))\n",
    "    return path\n",
    "\n",
    "@patch\n",
    "def load(path):\n",
    "    \"Load semantic layer from JSON and reconnect to filesystem\"\n",
    "    path = Path(path)\n",
    "    data = json.loads(path.read_text())\n",
    "    hyfs = HyFS()\n",
    "    \n",
    "    for eid, node_data in data['nodes'].items():\n",
    "        node_path = Path(node_data['path'])\n",
    "        if node_path.exists(): hyfs.add_node(node_path, eid=eid)\n",
    "    \n",
    "    for tag, eids in data['tags'].items():\n",
    "        for eid in eids:\n",
    "            if eid in hyfs.nodes: hyfs.tag(eid, tag)\n",
    "    \n",
    "    for eid, rels in data['relations'].items():\n",
    "        for rel_type, targets in rels.items():\n",
    "            for target in targets:\n",
    "                if eid in hyfs.nodes and target in hyfs.nodes:\n",
    "                    hyfs.relations[eid][rel_type].add(target)\n",
    "    \n",
    "    return hyfs\n",
    "\n",
    "HyFS.load = staticmethod(load)\n",
    "```\n",
    "\n",
    "Now let's add duplicate detection using `cid`:\n",
    "\n",
    "```python\n",
    "@patch\n",
    "def duplicates(self:HyFS):\n",
    "    \"Find files with identical content (same cid)\"\n",
    "    from collections import defaultdict\n",
    "    cid_to_nodes = defaultdict(list)\n",
    "    for node in self.nodes.values():\n",
    "        if node.type == 'file' and (c := node.cid): cid_to_nodes[c].append(node)\n",
    "    return {cid: L(nodes) for cid, nodes in cid_to_nodes.items() if len(nodes) > 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49102263",
   "metadata": {
    "solveit_ai": true
   },
   "source": [
    "Here's an exercise for you: I asked you the same question twice, but using different \"system prompts\" such that you would be much more concise the second time (in both cases, you did not see or even know about the other attempt).\n",
    "\n",
    "Compare these two implementations:\n",
    "\n",
    "- are there any major differences beyond coding style?\n",
    "- is there a difference in result?\n",
    "- is there a difference in computation time?\n",
    "- why seems superior for our specific project?\n",
    "\n",
    "\n",
    "##### ğŸ¤–ReplyğŸ¤–<!-- SOLVEIT_SEPARATOR_7f3a9b2c -->\n",
    "\n",
    "Great exercise! Let me compare the two implementations:\n",
    "\n",
    "**Major differences beyond style:**\n",
    "\n",
    "1. **Phantom nodes**: First version tracks entities that no longer exist on filesystem (`phantom=True`). Second version silently skips them.\n",
    "\n",
    "2. **CID caching**: First saves/loads `cid` values in JSON. Second recomputes from disk on load.\n",
    "\n",
    "3. **Load signature**: First is instance method (`hyfs.load(path)`), second is static/classmethod (`HyFS.load(path)`).\n",
    "\n",
    "4. **Atomic writes**: First uses temp file + rename pattern. Second writes directly.\n",
    "\n",
    "5. **Deduplication**: First includes full `deduplicate()` with strategies. Second only has `duplicates()` detection.\n",
    "\n",
    "6. **Error handling**: First has version checking and warnings. Second has none.\n",
    "\n",
    "**Difference in result:**\n",
    "\n",
    "- First preserves more information (phantom nodes, cached cids)\n",
    "- First is safer (atomic writes prevent corruption)\n",
    "- Second loses tags/relations for moved/deleted files\n",
    "\n",
    "**Computation time:**\n",
    "\n",
    "- First: saves cid â†’ faster on reload (no recompute)\n",
    "- Second: recomputes all cids on load â†’ slower but always fresh\n",
    "- Atomic write overhead in first is negligible\n",
    "\n",
    "**Which is superior for HyFS?**\n",
    "\n",
    "**Second version is better** for our specific project, despite being \"less complete\". Here's why:\n",
    "\n",
    "1. **Simpler is better at exploration phase** - We're in iteration 5, still figuring things out. The phantom nodes concept sounds clever but adds complexity we haven't proven we need.\n",
    "\n",
    "2. **CID recompute is actually correct** - Files change. Cached cids can lie. Better to recompute on load and trust the xattr cache for session performance.\n",
    "\n",
    "3. **Static load makes sense** - You're creating a new HyFS from saved state, not modifying existing. `HyFS.load(path)` reads more naturally than `hyfs.load(path)`.\n",
    "\n",
    "4. **Atomic writes are important but easy to add** - One line change when needed: `tmp = path.with_suffix('.tmp'); tmp.write_text(...); tmp.rename(path)`\n",
    "\n",
    "5. **Deduplication can be separate** - `duplicates()` returns the data structure. User can decide what to do. More Unix-y.\n",
    "\n",
    "The first version is what I'd build for production. The second is what you should build *now*. It does exactly what's needed, nothing more. You can add phantom nodes when you have a real use case for tracking deleted files. You can add `deduplicate()` when someone asks \"how do I delete these?\"\n",
    "\n",
    "**The fastcore way**: Build the minimum that works, make it composable, add features when they're needed. Second version nails this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efe0ed8",
   "metadata": {},
   "source": [
    "I totally agre, and truth be told I hadn't foreseen you would given the objective differences.\n",
    "\n",
    "This is really good fastcore style."
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "standard",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
